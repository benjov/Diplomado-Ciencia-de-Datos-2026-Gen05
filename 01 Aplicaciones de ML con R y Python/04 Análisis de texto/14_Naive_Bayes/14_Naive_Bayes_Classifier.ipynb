{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Naive Bayes Classifier in Python** <a class=\"anchor\" id=\"0\"></a>\n",
    "\n",
    "El clasificador Naïve Bayes es un algoritmo sencillo y potente para la tarea de clasificación. \n",
    "\n",
    "Usaremos las herramietas de Scikit-Learn para implementar el algoritmo de clasificación Naive Bayes.\n",
    "\n",
    "**Objetivo**: Construir un clasificador Naive Bayes para predecir si una persona gana más de 50K USD al año."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"0.1\"></a>\n",
    "# **Tabla de Contenido**\n",
    "\n",
    "1.\t[Introduction to Naive Bayes algorithm](#1)\n",
    "2.\t[Naive Bayes algorithm intuition](#2)\n",
    "3.\t[Types of Naive Bayes algorithm](#3)\n",
    "4.\t[Applications of Naive Bayes algorithm](#4)\n",
    "5.\t[Import libraries](#5)\n",
    "6.\t[Import dataset](#6)\n",
    "7.\t[Exploratory data analysis](#7)\n",
    "8.\t[Declare feature vector and target variable](#8)\n",
    "9.\t[Split data into separate training and test set](#9)\n",
    "10.\t[Feature engineering](#10)\n",
    "11.\t[Feature scaling](#11)\n",
    "12.\t[Model training](#12)\n",
    "13.\t[Predict the results](#13)\n",
    "14.\t[Check accuracy score](#14)\n",
    "15.\t[Confusion matrix](#15)\n",
    "16.\t[Classification metrices](#16)\n",
    "17.\t[Calculate class probabilities](#17)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction to Naive Bayes algorithm** <a class=\"anchor\" id=\"1\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "En el aprendizaje automático, el clasificador de Naive Bayes es un algoritmo sencillo y potente para la tarea de clasificación. \n",
    "\n",
    "El clasificador de Naive Bayes  se basa en la aplicación del teorema de Bayes con un fuerte supuesto de independencia entre las características. \n",
    "\n",
    "**El clasificador de Naive Bayes  produce buenos resultados cuando la usamos para análisis de datos textuales como el procesamiento del lenguaje natural.**\n",
    "\n",
    "Los modelos Naive Bayes también se conocen como \"Bayes simple\" o \"Bayes independiente\". Todos estos nombres hacen referencia a la aplicación del teorema de Bayes en la regla de decisión del clasificador. El clasificador Naive Bayes aplica el teorema de Bayes en la práctica. Este clasificador aporta el poder del teorema de Bayes al aprendizaje automático."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Naive Bayes algorithm intuition** <a class=\"anchor\" id=\"2\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "El clasificador Naïve Bayes utiliza el teorema de Bayes para predecir las probabilidades de membresía de cada clase, como la probabilidad de que un registro o punto de datos determinado pertenezca a una clase particular. \n",
    "\n",
    "La clase con mayor probabilidad se considera la clase más probable. Esto también se conoce como **Máximo A Posteriori (MAP)**.\n",
    "\n",
    "El Máximo A Posteriori con 2 eventos A y B es:\n",
    "\n",
    "$$MAP = max (P (A | B ) ) = max \\frac{P (B | A) * P (A)}{P (B)} = max (P (B | A) * P (A)) $$\n",
    "\n",
    "Aquí, $P(B)$ es la probabilidad de evidencia. Se utiliza para normalizar el resultado. Sigue siendo el mismo, por lo que eliminarlo no afectaría el resultado.\n",
    "\n",
    "El clasificador Naive Bayes supone que todas las características no están relacionadas entre sí. La presencia o ausencia de una característica no influye en la presencia o ausencia de cualquier otra característica.\n",
    "\n",
    "En conjuntos de datos del mundo real, probamos una hipótesis dada múltiples evidencias sobre las características. Entonces, los cálculos se vuelven bastante complicados. Para simplificar el trabajo, se utiliza el enfoque de independencia de características para desacoplar múltiples pruebas y tratar cada una como independiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Types of Naive Bayes algorithm** <a class=\"anchor\" id=\"3\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "\n",
    "Hay 3 tipos de algoritmo Naïve Bayes. Los 3 tipos se enumeran a continuación: -\n",
    "\n",
    "  1. Gaussian Naïve Bayes\n",
    "\n",
    "  2. Multinomial Naïve Bayes\n",
    "\n",
    "  3. Bernoulli Naïve Bayes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gaussian Naïve Bayes algorithm**\n",
    "\n",
    "Cuando tenemos valores de atributos continuos, asumimos que los valores asociados con cada clase se distribuyen según una distribución gaussiana o normal. Por ejemplo, supongamos que los datos de entrenamiento contienen un atributo continuo $x$. Primero segmentamos los datos por clase y luego calculamos la media y la varianza de x en cada clase. Sea µi la media de los valores y sea $σ_i$ la varianza de los valores asociados con la iésima clase. Supongamos que tenemos algún valor de observación xi. Entonces, la distribución de probabilidad de $x_i$ dada una clase se puede calcular mediante la siguiente ecuación:\n",
    "\n",
    "![Gaussian Naive Bayes algorithm](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQEWCcq1XtC1Yw20KWSHn2axYa7eY-a0T1TGtdVn5PvOpv9wW3FeA&s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multinomial Naïve Bayes algorithm**\n",
    "\n",
    "Con un modelo Multinomial Naïve Bayes, las muestras (vectores de características) representan las frecuencias con las que ciertos eventos han sido generados por un multinomial ($p_1, . . . ,p_n$) donde $p_i$ es la probabilidad de que ocurra el evento $i$. \n",
    "\n",
    "Se prefiere utilizar el algoritmo multinomial Naïve Bayes en datos con distribución multinomial. \n",
    "\n",
    "Es uno de los algoritmos estándar que se utiliza en la clasificación de categorización de texto."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Bernoulli Naïve Bayes algorithm**\n",
    "\n",
    "En el modelo de eventos multivariado de Bernoulli, las características son variables booleanas independientes (variables binarias) que describen las entradas. \n",
    "\n",
    "Al igual que el modelo multinomial, este modelo también es popular para tareas de clasificación de documentos donde se utilizan características de aparición de términos binarios en lugar de frecuencias de términos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Applications of Naive Bayes algorithm** <a class=\"anchor\" id=\"4\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Naïve Bayes es uno de los algoritmos de clasificación más sencillos y rápidos. Es muy adecuado para grandes volúmenes de datos. \n",
    "\n",
    "Se utiliza con éxito en diversas aplicaciones como:\n",
    "1. Filtrado de spam\n",
    "2. Clasificación de textos\n",
    "3. Análisis de sentimiento\n",
    "4. Sistemas de recomendación\n",
    "\n",
    "Utiliza el teorema de probabilidad de Bayes para la predicción de clases desconocidas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Import libraries** <a class=\"anchor\" id=\"5\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "from sklearn.model_selection import train_test_split\n",
    "import category_encoders as ce\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install category_encoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Import dataset** <a class=\"anchor\" id=\"6\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "df = pd.read_csv( 'adult.csv', header = None )\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **7. Exploratory data analysis** <a class=\"anchor\" id=\"7\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view dimensions of dataset\n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename column names\n",
    "\n",
    "col_names = ['age', 'workclass', 'fnlwgt', 'education', 'education_num', 'marital_status', 'occupation', 'relationship',\n",
    "             'race', 'sex', 'capital_gain', 'capital_loss', 'hours_per_week', 'native_country', 'income']\n",
    "\n",
    "df.columns = col_names\n",
    "\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's again preview the dataset\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view summary of dataset\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resumen de las variables categoricas\n",
    "\n",
    "- Hay 9 variables categoricas.\n",
    "\n",
    "\n",
    "- Estas son: `workclass`, `education`, `marital_status`, `occupation`, `relationship`, `race`, `sex`, `native_country`, `income`.\n",
    "\n",
    "\n",
    "- `income` es nuestra variable objetivo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables\n",
    "\n",
    "categorical = [ 'workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', \n",
    "                 'sex', 'native_country', 'income']\n",
    "\n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency counts of values in categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    print( df[var].value_counts() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view frequency distribution of categorical variables\n",
    "\n",
    "for var in categorical: \n",
    "    \n",
    "    print(df[var].value_counts(normalize = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notemos que hay variables `workclass`, `occupation` y `native_country` que contienen missing values (denominados como `?`). \n",
    "\n",
    "Generalmente, los missing values se codifican como `NaN` y es sencillo detectarlos con Python (`df.isnull().sum()`).\n",
    "\n",
    "Reemplazaremos `?` con `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in workclass variable\n",
    "\n",
    "df.workclass.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in workclass variable\n",
    "\n",
    "df.workclass.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' values in workclass variable with `NaN`\n",
    "\n",
    "\n",
    "df['workclass'].replace(' ?', np.NaN, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# again check the frequency distribution of values in workclass variable\n",
    "\n",
    "df.workclass.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in occupation variable\n",
    "\n",
    "df.occupation.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in occupation variable\n",
    "\n",
    "df.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' values in occupation variable with `NaN`\n",
    "\n",
    "df['occupation'].replace(' ?', np.NaN, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again check the frequency distribution of values in occupation variable\n",
    "\n",
    "df.occupation.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check labels in native_country variable\n",
    "\n",
    "df.native_country.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check frequency distribution of values in native_country variable\n",
    "\n",
    "df.native_country.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace '?' values in native_country variable with `NaN`\n",
    "\n",
    "df['native_country'].replace(' ?', np.NaN, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# again check the frequency distribution of values in native_country variable\n",
    "\n",
    "df.native_country.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check missing values in categorical variables again\n",
    "\n",
    "df[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for cardinality in categorical variables\n",
    "\n",
    "for var in categorical:\n",
    "    \n",
    "    print(var, ' contains ', len(df[var].unique()), ' labels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the numerical variables\n",
    "\n",
    "numerical = [ 'age', 'fnlwgt', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week' ]\n",
    "\n",
    "df[numerical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in numerical variables\n",
    "\n",
    "df[numerical].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **8. Declare feature vector and target variable** <a class=\"anchor\" id=\"8\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "X = df.drop( ['income'], axis = 1 )\n",
    "\n",
    "y = df['income']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **9. Split data into separate training and test set** <a class=\"anchor\" id=\"9\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split X and y into training and testing sets\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the shape of X_train and X_test\n",
    "\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **10. Feature Engineering** <a class=\"anchor\" id=\"10\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "**Feature Engineering** es el proceso de transformar datos sin procesar en funciones útiles que nos ayudan a comprender mejor nuestro modelo y aumentar su poder predictivo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check data types in X_train\n",
    "\n",
    "X_train.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print percentage of missing values in the categorical variables in training set\n",
    "\n",
    "categorical = [ 'workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', \n",
    "                 'sex', 'native_country' ]\n",
    "\n",
    "X_train[categorical].isnull().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print categorical variables with missing data\n",
    "\n",
    "for col in categorical:\n",
    "    if X_train[col].isnull().mean()>0:\n",
    "        print(col, (X_train[col].isnull().mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing categorical variables with most frequent value\n",
    "\n",
    "for df2 in [X_train, X_test]:\n",
    "    df2['workclass'].fillna(X_train['workclass'].mode()[0], inplace=True)\n",
    "    df2['occupation'].fillna(X_train['occupation'].mode()[0], inplace=True)\n",
    "    df2['native_country'].fillna(X_train['native_country'].mode()[0], inplace=True)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_train\n",
    "\n",
    "X_train[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in categorical variables in X_test\n",
    "\n",
    "X_test[categorical].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_train\n",
    "\n",
    "X_train.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check missing values in X_test\n",
    "\n",
    "X_test.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "X_train[categorical].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode categorical variables\n",
    "# encode remaining variables with one-hot encoding\n",
    "\n",
    "encoder = ce.OneHotEncoder( cols = [ 'workclass', 'education', 'marital_status', 'occupation', \n",
    "                                     'relationship', 'race', 'sex', 'native_country' ] )\n",
    "\n",
    "X_train = encoder.fit_transform(X_train)\n",
    "\n",
    "X_test = encoder.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **11. Feature Scaling** <a class=\"anchor\" id=\"11\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "cols = X_train.columns\n",
    "\n",
    "cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. ¿Qué es RobustScaler()?\n",
    "RobustScaler es una clase de la biblioteca scikit-learn (sklearn), que se utiliza para escalar o normalizar datos numéricos.\n",
    "La normalización o escalado es una etapa clave en machine learning porque muchos algoritmos funcionan mejor (o directamente requieren) que los datos numéricos estén en una escala similar.\n",
    "RobustScaler escala los datos usando la mediana y el rango intercuartílico (IQR: el rango entre el percentil 25 y el 75).\n",
    "Ventaja: Es robusto ante outliers (valores atípicos/extremos), porque la mediana y los cuartiles no se ven tan afectados por ellos como la media y la desviación estándar (que usa StandardScaler).\n",
    "\n",
    "2. ¿Qué hace .fit_transform(X_train)?\n",
    "scaler.fit_transform(X_train):\n",
    "fit: Calcula la mediana y los cuartiles de cada columna de X_train (tu conjunto de entrenamiento).\n",
    "transform: Usando esos valores calculados, transforma todos los valores de X_train para que estén en la nueva escala.\n",
    "El resultado es que X_train queda escalado, es decir, transformado para que los outliers tengan mucho menos efecto.\n",
    "\n",
    "3. ¿Qué hace .transform(X_test)?\n",
    "scaler.transform(X_test):\n",
    "Toma los parámetros (mediana y cuartiles) que aprendió usando X_train y transforma el conjunto de prueba (X_test) usando esos mismos valores.\n",
    "Importante:\n",
    "**Nunca debes “aprender” los parámetros del test, sólo del entrenamiento, para evitar fugas de información (data leakage).**\n",
    "\n",
    "4. ¿Por qué hacerlo así?\n",
    "Porque así garantizas que ambos conjuntos están en la misma escala, pero el test nunca influye en el cálculo del escalado (lo cual sería un error).\n",
    "Esto hace que tus modelos sean más robustos y se generalicen mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "scaler = RobustScaler()\n",
    "\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_train = pd.DataFrame(X_train, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_test = pd.DataFrame(X_test, columns=[cols])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **12. Model training** <a class=\"anchor\" id=\"12\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a Gaussian Naive Bayes classifier on the training set\n",
    "\n",
    "# instantiate the model\n",
    "gnb = GaussianNB()\n",
    "\n",
    "# fit the model\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **13. Predict the results** <a class=\"anchor\" id=\"13\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **14. Check accuracy score** <a class=\"anchor\" id=\"14\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "print('Test-set accuracy score: {0:0.4f}'. format(accuracy_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "y_pred_train = gnb.predict(X_train)\n",
    "\n",
    "y_pred_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Training-set accuracy score: {0:0.4f}'. format(accuracy_score(y_train, y_pred_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check for overfitting and underfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the scores on training and test set\n",
    "\n",
    "print('Training set score: {:.4f}'.format(gnb.score(X_train, y_train)))\n",
    "\n",
    "print('Test set score: {:.4f}'.format(gnb.score(X_test, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**La precisión del conjunto de entrenamiento es 0,8067, mientras que la precisión del conjunto de prueba es 0,8083.**\n",
    "\n",
    "**Estos dos valores son bastante comparables. Por tanto, no hay signos de sobreajuste.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **15. Confusion matrix** <a class=\"anchor\" id=\"15\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)\n",
    "\n",
    "Una **matriz de confusión** es una herramienta para resumir el desempeño de un algoritmo de clasificación. Una matriz de confusión nos dará una imagen clara del rendimiento del modelo de clasificación y los tipos de errores producidos por el modelo. Nos da un resumen de predicciones correctas e incorrectas desglosadas por cada categoría. \n",
    "\n",
    "El resumen se presenta en forma de tabla. Son posibles cuatro tipos de resultados. Estos cuatro resultados se describen a continuación:\n",
    "\n",
    "**Verdaderos positivos (TP)**: los verdaderos positivos ocurren cuando predecimos que una observación pertenece a una determinada clase y la observación en realidad pertenece a esa clase.\n",
    "\n",
    "**Verdaderos Negativos (TN)** – Los Verdaderos Negativos ocurren cuando predecimos que una observación no pertenece a una determinada clase y la observación en realidad no pertenece a esa clase.\n",
    "\n",
    "**Falsos positivos (FP)**: los falsos positivos ocurren cuando predecimos que una observación pertenece a una determinada clase, pero la observación en realidad no pertenece a esa clase. Este tipo de error se llama **Error tipo I.**\n",
    "\n",
    "**Falsos negativos (FN)**: los falsos negativos ocurren cuando predecimos que una observación no pertenece a una determinada clase, pero la observación en realidad pertenece a esa clase. Este es un error muy grave y se llama **Error de tipo II.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the Confusion Matrix and slice it into four pieces\n",
    "\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "print('Confusion matrix\\n\\n', cm)\n",
    "TP = cm[0,0]\n",
    "print('\\nTrue Positives(TP) = ', cm[0,0])\n",
    "TN = cm[1,1]\n",
    "print('\\nTrue Negatives(TN) = ', cm[1,1])\n",
    "FP = cm[0,1]\n",
    "print('\\nFalse Positives(FP) = ', cm[0,1])\n",
    "FN = cm[1,0]\n",
    "print('\\nFalse Negatives(FN) = ', cm[1,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize confusion matrix with seaborn heatmap\n",
    "\n",
    "cm_matrix = pd.DataFrame( data = cm, \n",
    "                          columns = [ 'Actual Positive: 1', 'Actual Negative: 0' ],\n",
    "                          index = [ 'Predict Positive: 1', 'Predict Negative: 0' ])\n",
    "\n",
    "sns.heatmap( cm_matrix, annot = True, fmt = 'd', cmap = 'YlGnBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **16. Classification metrices** <a class=\"anchor\" id=\"16\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Informe de clasificación** es otra forma de evaluar el rendimiento del modelo de clasificación. Muestra las puntuaciones de **precisión (precision)**, **recuperación (recall)**, **f1** y **soporte (support)** para el modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print precision score\n",
    "\n",
    "precision = TP / float(TP + FP)\n",
    "\n",
    "\n",
    "print('Precision : {0:0.4f}'.format(precision))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "recall = TP / float(TP + FN)\n",
    "\n",
    "print('Recall or Sensitivity : {0:0.4f}'.format(recall))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "false_positive_rate = FP / float(FP + TN)\n",
    "\n",
    "\n",
    "print('False Positive Rate : {0:0.4f}'.format(false_positive_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "\n",
    "specificity = TN / (TN + FP)\n",
    "\n",
    "print('Specificity : {0:0.4f}'.format(specificity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  f1 Score\n",
    "\n",
    "**f1-score** es la media armónica ponderada de precisión y recuperación. El mejor **f1-score** posible sería 1.0 y el peor sería 0.0. \n",
    "\n",
    "$$f1-Score = \\frac{2 \\times precision \\times recall}{precision + recall}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **17. Calculate class probabilities** <a class=\"anchor\" id=\"17\"></a>\n",
    "\n",
    "[Table of Contents](#0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print the first 10 predicted probabilities of two classes- 0 and 1\n",
    "\n",
    "y_pred_prob = gnb.predict_proba(X_test)[0:10]\n",
    "\n",
    "y_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the probabilities in dataframe\n",
    "\n",
    "y_pred_prob_df = pd.DataFrame(data=y_pred_prob, columns=['Prob of - <=50K', 'Prob of - >50K'])\n",
    "\n",
    "y_pred_prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store the predicted probabilities for class 1 - Probability of >50K\n",
    "\n",
    "y_pred1 = gnb.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot histogram of predicted probabilities\n",
    "\n",
    "\n",
    "# adjust the font size \n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "\n",
    "# plot histogram with 10 bins\n",
    "plt.hist(y_pred1, bins = 10)\n",
    "\n",
    "\n",
    "# set the title of predicted probabilities\n",
    "plt.title('Histogram of predicted probabilities of salaries >50K')\n",
    "\n",
    "\n",
    "# set the x-axis limit\n",
    "plt.xlim(0,1)\n",
    "\n",
    "\n",
    "# set the title\n",
    "plt.xlabel('Predicted probabilities of salaries >50K')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Go to Top](#0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
