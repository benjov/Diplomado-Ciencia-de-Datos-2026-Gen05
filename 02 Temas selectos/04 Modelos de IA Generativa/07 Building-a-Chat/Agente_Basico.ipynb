{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_xc80nh6oaE"
   },
   "source": [
    "# Ejemplo 1 - Agente de IA b치sico con LangChain y Groq\n",
    "\n",
    "## Casos de uso: Chat simple\n",
    "\n",
    "Objetivo: Crear un agente b치sico que responda preguntas generales usando un LLM open source v칤a Groq.\n",
    "Framework: LangChain\n",
    "\n",
    "### Paso a paso\n",
    "\n",
    "| Paso | Acci칩n |\n",
    "| --- | --- |\n",
    "| 1 | Crear una peque침a herramienta. | |\n",
    "|  | Usamos la clase tool que envuelve una funci칩n Python. |\n",
    "| 2 | Inicializar el agente con initialize_agent. |\n",
    "|  | Utilizamos el tipo zero-shot-react-description, que |\n",
    "|  | permite al modelo razonar paso a paso para decidir usar |\n",
    "|  | o no herramientas. |\n",
    "| 3 | Probarlo. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1_fjtsT29BvK"
   },
   "source": [
    "# \t1.\tEntorno Colab\n",
    "\n",
    "Ejecuta primero (una sola vez) el siguiente bloque para instalar todo lo necesario:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1B3zdLr86nY5",
    "outputId": "7797433f-7f5e-426e-f859-37c1d41f85b2"
   },
   "outputs": [],
   "source": [
    "# Instalaci칩n de dependencias necesarias para Colab\n",
    "!pip install -q langchain langchain-groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WfXICOlpFLoJ"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Importar librer칤as\n",
    "# ========================================\n",
    "from langchain_groq import ChatGroq\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1CCi0xR96Ol"
   },
   "source": [
    "# 2.\tClave de Groq\n",
    "\n",
    "En Colab guarda tu clave como variable de entorno.\n",
    "\n",
    "Para obtenerla visita: https://groq.com/\n",
    "\n",
    "**no queda en el notebook si la escribes con getpass**\n",
    "\n",
    "**puedes dejarlo visible en el notebook**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v9-I0jJZ95na"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Configurar la API de Groq\n",
    "# ========================================\n",
    "\n",
    "import os, getpass\n",
    "#os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"游댐 Ingresa tu Groq API key: \")\n",
    "os.environ[\"GROQ_API_KEY\"] = \"...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQWEWDRcABEz"
   },
   "source": [
    "#3.\tImportar 'motor' LLM\n",
    "\n",
    "Puedes seleccionar otros modelos disponibles en https://console.groq.com/docs/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d9Pp9jFO6YlG"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Crear el modelo LLM\n",
    "# ========================================\n",
    "# Usamos ChatGroq como wrapper para acceder al modelo v칤a LangChain\n",
    "from langchain_groq import ChatGroq\n",
    "#Models: 'openai/gpt-oss-120b', 'llama-3.3-70b-versatile'\n",
    "llm = ChatGroq(\n",
    "    model_name='llama-3.3-70b-versatile', # cualquier modelo open-source disponible\n",
    "    temperature=0.2, # respuestas m치s deterministas\n",
    "    max_tokens=20000,\n",
    "    streaming=True                  # stream para menor latencia\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pv-xqhoyBmg_"
   },
   "source": [
    "El wrapper ChatGroq implementa la interfaz est치ndar de LangChain, as칤 que\n",
    "funciona con cadenas, agentes y RAG sin cambiar m치s c칩digo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mpC2UCOW6YnZ",
    "outputId": "0dd25394-6c76-421f-b87e-a2e29e983c3e"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Memoria para mantener el contexto\n",
    "# ========================================\n",
    "memory = ConversationBufferMemory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1w1rzXyz6Yp8",
    "outputId": "af21d353-e803-4a8b-f3ee-a6436f4f6596"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Cadena de conversaci칩n\n",
    "# ========================================\n",
    "conversation = ConversationChain(\n",
    "    llm=llm,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PqrWUXMB6Ysi",
    "outputId": "6a6e92de-d683-4d0b-e6d1-1fcba904ba3d"
   },
   "outputs": [],
   "source": [
    "# ========================================\n",
    "# Interactuar con el agente\n",
    "# ========================================\n",
    "print(\"=== Agente de IA B치sico ===\")\n",
    "while True:\n",
    "    pregunta = input(\"T칰: \")\n",
    "    if pregunta.lower() in [\"salir\", \"exit\"]:\n",
    "        print(\"Agente: 춰Hasta luego!\")\n",
    "        break\n",
    "    respuesta = conversation.run(pregunta)\n",
    "    print(f\"Agente: {respuesta}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K_ZVPABY6Yxx"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
